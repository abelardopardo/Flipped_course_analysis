---
title: 'R Notebook: Examining student sessions'
output:
  html_notebook: default
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Compute weekly session counts per student

Start by loading the data about learning actions during active course weeks (2-13); the data are loaded from the *Intermediate_results/trace_data_with_sessions_w2-13.RData* file.
```{r}
traces <- readRDS(file = "Intermediate_results/trace_data_with_sessions_w2-13.RData")
str(traces)
```
The file contains data about `r nrow(traces)` learning actions in total, done by `r length(unique(traces$USER_ID))` students.

Next, compute the number of sessions per student and per week; create a matrix with students in the rows and weekly session counts in the columns.
```{r}
stud.ids <- unique(traces$USER_ID)
n.stud <- length(stud.ids)
sessions.m <- matrix(nrow = n.stud, ncol = 13, byrow = T,
                     data = rep(x = 0, times=(n.stud*13))) 
for(i in 1:n.stud) {
  stud.data <- subset(traces, USER_ID==stud.ids[i])
  weekly.counts <- tapply(X = stud.data$SESSION_ID, INDEX = stud.data$WEEK,
                          FUN = function(x) {length(unique(x))})
  weekly.counts.df <- data.frame(week=as.integer(unlist(dimnames(weekly.counts))), 
                                 freq=as.vector(weekly.counts), stringsAsFactors = F)
  
  ## if the data is available for all the course weeks
  if (nrow(weekly.counts.df)==12) { 
    sessions.m[i,] <- c(stud.ids[i], weekly.counts.df$freq)
    next
  }
  
  ## if the data (sessions) is not available for all the course weeks
  ## find the weeks that lack the data (sessions)
  missing.weeks <- setdiff(c(2:13), weekly.counts.df$week)
  ## add zero frequency for such weeks
  for(j in 1:length(missing.weeks)) {
    weekly.counts.df <- as.data.frame(rbind(weekly.counts.df, c(missing.weeks[j],0)))
  }
  ## now, sort the weekly.counts.df based on the week column so that
  ## weekly counts (frequences) are given in the expected order
  weekly.counts.df <- weekly.counts.df[ order(weekly.counts.df$week, decreasing = F),]
  sessions.m[i,] <- c(stud.ids[i], weekly.counts.df$freq)
}
```

The matrix with students' weekly session counts (first few rows):
```{r}
head(sessions.m)
```

Transform the matrix into a data frame
```{r}
sessions.df <- as.data.frame(x = sessions.m, row.names = NULL, stringsAsFactors = F)
colnames(sessions.df) <- c("USER_ID", paste0("W", c(2:13)))
str(sessions.df)
```

Save the computed weekly counts to the file *Intermediate_results/weekly_per_student_session_counts.RData*
```{r}
saveRDS(object = sessions.df, 
        file = "Intermediate_results/weekly_per_student_session_counts.RData")
```

## Examine student profiles / groups based on the weekly session counts

The idea is to use the computed weekly session counts as the input for LCA or a clustering method to detect the presence of students with similar pattern of engagement during the course weeks

### Transform weekly session counts into multinomial variables

To use LCA, the weekly counts variables need to be transformed into multinomial variables. 
To that end, the counts variables will be dicretized, and turned into factor variables with 5 possible values: 

* 0 - student had no sessions in the given week 
* 1 - the student's number of sessions belongs to the 1st quartile for the given week 
* 2 - the student's number of sessions belongs to the 2nd quartile for the given week 
* 3 - the student's number of sessions belongs to the 3rd quartile for the given week 
* 4 - the student's number of sessions belongs to the 4th quartile for the given week

Create a new data frame with multinomial variables as described above:
```{r}
multinom.sdata <- sessions.df
for(j in 2:13) {
  q <- quantile(x = sessions.df[,j], probs = c(0.25, 0.5, 0.75), names = F)
  multinom.sdata[,j][multinom.sdata[,j] > 0 & multinom.sdata[,j] <= q[1]] <- 1
  multinom.sdata[,j][multinom.sdata[,j] > q[1] & multinom.sdata[,j] <= q[2]] <- 2
  multinom.sdata[,j][multinom.sdata[,j] > q[2] & multinom.sdata[,j] <= q[3]] <- 3
  multinom.sdata[,j][multinom.sdata[,j] > q[3]] <- 4
} 

#multinom.sdata[,c(2:13)] <- as.data.frame(apply(multinom.sdata[,c(2:13)], 2, factor))
```
 
```{r}
str(multinom.sdata)
head(multinom.sdata)
```

### Use LCA to group students based on the session distribution throughout the course

```{r message=FALSE}
library(poLCA)
library(knitr)
```

To determine the best value for the number of LCA classes, create models for different number of classes (2-7), and compute several statistical indicators (AIC, BIC, aBIC, cAIC) for each model

```{r}
for(j in 2:13) {
  multinom.sdata[,j] <- multinom.sdata[,j] + 1
}

weekly.sessions <- cbind(W2, W3, W4, W5, W6, W7, W8, W9, W10, W11, W12, W13) ~ 1

eval.metrics <- data.frame()
set.seed(25032017)
for(i in 2:7) {
  lc <- poLCA(weekly.sessions, data = multinom.sdata, nrep = 50, 
              nclass = i, verbose = F, na.rm = F)
  metrics <- c(nclass = i, 
               AIC = lc$aic, 
               BIC = lc$bic, 
               ABIC = (-2*lc$llik) + ((log((lc$N + 2)/24)) * lc$npar),
               CAIC = (-2*lc$llik) + lc$npar * (1 + log(lc$N)),
               LogLike = lc$llik, #log likelihood 
               ChiSq = lc$Chisq) # Chi square
  eval.metrics <- as.data.frame( rbind(eval.metrics, metrics) )
}
colnames(eval.metrics) <- c('nclass', 'AIC', 'BIC', 'ABIC', 'CAIC', 'LogLike', 'ChiSquare')
```

```{r results="asis"}
kable(x = eval.metrics, format = "pandoc")
```

#### Examine the model with 3 classes (BIC and cAIC drop and then rise again)

```{r include=FALSE}
set.seed(25032017)
lc3 <- poLCA(weekly.sessions, data = multinom.sdata, nrep = 50, 
             nclass = 3, graphs = F, na.rm = F)
```

```{r}
probs.start <- poLCA.reorder(lc3$probs.start, order(lc3$P,decreasing=T))
lc3 <- poLCA(weekly.sessions, data = multinom.sdata, graphs = F, 
             nclass = 3, probs.start = probs.start, na.rm = F)
```

Add the predicted class to each observation
```{r}
lc3.results <- multinom.sdata
lc3.results$LCA_CL <- as.factor(lc3$predclass)
```

Examine class counts and proportions
```{r}
table(lc3.results$LCA_CL)
round(prop.table(table(lc3.results$LCA_CL)), digits = 3)
```

Save the features and the LCA class values to the file *results/LCA_based_on_weekly_session_counts.csv*
```{r}
write.csv(x = lc3.results, file = "results/LCA_based_on_weekly_session_counts.csv",
          quote = F, row.names = F)
```

Plot the classes in the manner suggested in [this blog post](http://statistics.ohlsen-web.de/latent-class-analysis-polca/)

```{r results="hide", message=FALSE}
require(reshape2)

lc3.model <- reshape2::melt(lc3$probs)
lc3.model$L1 <- factor(lc3.model$L1, 
                       levels = c('W2','W3','W4','W5','W6','W7',
                                  'W8','W9','W10','W11','W12','W13'))
lc3.model$Var2 <- factor(lc3.model$Var2, 
                         labels = c('NONE','Q1','Q2','Q3','Q4'))
lc3.model$Var1 <- factor(lc3.model$Var1, 
                         labels = c('group 1','group 2','group 3'))
str(lc3.model)
```

```{r message=FALSE, out.height = "10in", out.width = "7in", fig.cap="Probability distribution of session count quartiles across the course weeks and the 3 LCA groups"}
require(ggplot2)
#custom.pallet <- c('#bebada','#ffffb3', '#80b1d3', '#8dd3c7', '#fb8072')
zp1 <- ggplot(lc3.model,aes(x = L1, y = value, fill = Var2))
zp1 <- zp1 + geom_bar(stat = "identity", position = "stack")
zp1 <- zp1 + facet_grid(Var1 ~ .) 
#zp1 <- zp1 + scale_fill_manual(values=custom.pallet) 
zp1 <- zp1 + scale_fill_brewer() + theme_bw()
zp1 <- zp1 + labs(x = "Course weeks", y="Probability", fill ="Session count quartiles")
zp1 <- zp1 + theme( axis.text.y=element_blank(),
                    axis.ticks.y=element_blank(),                    
                    panel.grid.major.y=element_blank())
zp1 <- zp1 + guides(fill = guide_legend(reverse=TRUE))
print(zp1)
```


### Examine exam scores of students from different (LCA) groups

Load exam scores data and merge it with the identified LCA classes
```{r}
all.vars <- read.csv(file = "dataset/data2u_sem2_15_student_all_variables.csv")
scores <- all.vars[,c("user_id", "SC_MT_TOT", "SC_FE_TOT")]
remove(all.vars)
## remove students who do not have exam scores
na.scores <- which(is.na(scores$SC_MT_TOT) | is.na(scores$SC_FE_TOT))
scores <- scores[-na.scores,]

lca.and.scores <- merge(x = lc3.results[,c(1,14)], y = scores,
                        by.x = "USER_ID", by.y = "user_id", 
                        all.x = F, all.y = F)
str(lca.and.scores)
```

Exam scores data are not available for all the students with learning actions data; in particular, `r nrow(lca.and.scores)` students have scores.  

Compute the basic stats for the midterm and final exams
```{r echo=TRUE}
mt.stats <- quantile(x = lca.and.scores$SC_MT_TOT, probs = c(0.25,0.5,0.75))
mt.stats
```

```{r echo=TRUE}
fe.stats <- quantile(x = lca.and.scores$SC_FE_TOT, probs = c(0.25,0.5,0.75))
fe.stats
```



#### Check if groups differ significantly w.r.t. the **midterm exam** score

Use [Kruskal-Wallis test](http://www.r-tutor.com/elementary-statistics/non-parametric-methods/kruskal-wallis-test) to check for significant difference among the groups

```{r}
with(lca.and.scores, kruskal.test(SC_MT_TOT ~ LCA_CL))
```


Use [Mann-Whitney U Test](http://www.r-tutor.com/elementary-statistics/non-parametric-methods/mann-whitney-wilcoxon-test) to do pair-wise comparisons of LCA groups based on student midterm scores

```{r message=FALSE}
source(file = "util_functions.R")
colnames(lca.and.scores)[2] <- "class"
pairwise.mtxam.compare <- function(nclust, ncomparison, hcl.plus.scores) {
  comparison <- matrix(nrow = ncomparison, ncol = 5, byrow = T)
  k <- 1
  for(i in 1:(nclust-1)) {
    for(j in (i+1):nclust) {
      comparison[k,] <- c(i, j, compare.midterm.scores.Mann.Whitney.test(hcl.plus.scores, i, j))
      k <- k+1
    }
  }
  comparison.df <- as.data.frame(comparison)
  colnames(comparison.df) <- c('c1', 'c2', 'Z', 'p', 'r')
  comparison.df <- apply.FDR.correction(comparison.df)
  comparison.df
}
kable(pairwise.mtxam.compare(3, 3, lca.and.scores), format = "markdown")
```



#### Check if groups differ significantly w.r.t. the **final exam** score

Use Kruskal-Wallis test to check for the difference among the groups

```{r}
with(lca.and.scores, kruskal.test(SC_FE_TOT ~ class))
```


Now, apply Mann-Whitney U Test to do pair-wise comparisons of the LCA groups
```{r echo=FALSE}
pairwise.fexam.compare <- function(nclust, ncomparison, hcl.plus.scores) {
  comparison <- matrix(nrow = ncomparison, ncol = 5, byrow = T)
  k <- 1
  for(i in 1:(nclust-1)) {
    for(j in (i+1):nclust) {
      comparison[k,] <- c(i, j, compare.fexam.scores.Mann.Whitney.test(hcl.plus.scores, i, j))
      k <- k+1
    }
  }
  comparison.df <- as.data.frame(comparison)
  colnames(comparison.df) <- c('c1', 'c2', 'Z', 'p', 'r')
  comparison.df <- apply.FDR.correction(comparison.df)
  comparison.df
}
kable(pairwise.fexam.compare(3, 3, lca.and.scores), format = "pandoc")
```


#### Examine more closely the students from the least active group (group 1) who have high scores

```{r echo=FALSE}
to.examine <- lca.and.scores$USER_ID[lca.and.scores$class==1 &
                                       lca.and.scores$SC_MT_TOT > as.numeric(mt.stats[2]) &
                                       lca.and.scores$SC_FE_TOT > as.numeric(fe.stats[2])]

```
There are **`r length(to.examine)`** students from the least active group with both midterm and final exam scores aboe the median values.


```{r}
## merge session counts and scores to examine the selected students
counts.and.scores <- merge(x = sessions.df, y = lca.and.scores,
                           by = "USER_ID", all.x = F, all.y = T)
counts.and.scores$SC_FE_TOT <- round(counts.and.scores$SC_FE_TOT, digits = 2)
group1.stats <- counts.and.scores[counts.and.scores$USER_ID %in% to.examine, c(1:13,15,16)]
kable(group1.stats, format = "pandoc")
```

